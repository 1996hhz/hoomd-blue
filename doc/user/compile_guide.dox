/*
Highly Optimized Object-oriented Many-particle Dynamics -- Blue Edition
(HOOMD-blue) Open Source Software License Copyright 2009-2016 The Regents of
the University of Michigan All rights reserved.

HOOMD-blue may contain modifications ("Contributions") provided, and to which
copyright is held, by various Contributors who have granted The Regents of the
University of Michigan the right to modify and/or distribute such Contributions.

You may redistribute, use, and create derivate works of HOOMD-blue, in source
and binary forms, provided you abide by the following conditions:

* Redistributions of source code must retain the above copyright notice, this
list of conditions, and the following disclaimer both in the code and
prominently in any materials provided with the distribution.

* Redistributions in binary form must reproduce the above copyright notice, this
list of conditions, and the following disclaimer in the documentation and/or
other materials provided with the distribution.

* All publications and presentations based on HOOMD-blue, including any reports
or published results obtained, in whole or in part, with HOOMD-blue, will
acknowledge its use according to the terms posted at the time of submission on:
http://codeblue.umich.edu/hoomd-blue/citations.html

* Any electronic documents citing HOOMD-Blue will link to the HOOMD-Blue website:
http://codeblue.umich.edu/hoomd-blue/

* Apart from the above required attributions, neither the name of the copyright
holder nor the names of HOOMD-blue's contributors may be used to endorse or
promote products derived from this software without specific prior written
permission.

Disclaimer

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDER AND CONTRIBUTORS ``AS IS'' AND
ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, AND/OR ANY
WARRANTIES THAT THIS SOFTWARE IS FREE OF INFRINGEMENT ARE DISCLAIMED.

IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT,
INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING,
BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE
OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF
ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
*/



/*!
\page page_compile_guide Compiling HOOMD-blue

Table of contents:
- \ref sec_software_req
    - \ref sec_software_req_clusters
    - \ref sec_building_boost
    - \ref sec_building_workstation
- \ref sec_build_linux_generic_compile
- \ref sec_build_linux_generic_mpi
- \ref sec_build_options
- \ref sec_build_plugin
<hr>

\section sec_software_req Software Prerequisites

HOOMD-blue requires a number of prerequisite software packages and libraries.

 * Required:
     * Python >= 2.6
     * numpy >= 1.7
     * boost >= 1.39.0
     * CMake >= 2.6.2
     * C++ Compiler (tested with gcc, clang, intel)
 * Optional:
     * NVIDIA CUDA Toolkit >= 5.0
     * MPI (tested with OpenMPI, MVAPICH, impi)
 * Useful developer tools
     * Git >= 1.7.0
     * Doxygen  >= 1.8.5

See \ref sec_mpi_best_practices for a discussion of which kind of MPI library is best for your situation
See \ref sec_build_linux_generic_mpi for instructions on building an MPI enabled hoomd.

\subsection sec_software_req_clusters Loading software prerequisites on clusters

Most cluster administrators provide versions of python, numpy, mpi, and cuda as modules. Some provide boost, and a few
provide boost with a working boost::python. Here are the module commands necessary to load prerequisites at national
supercomputers. Each code block also specifies a recommended install location ${SOFTWARE_ROOT} where hoomd can
be loaded on the compute nodes with minimal file system impact. On many clusters, administrators will block your account
without warning if you launch hoomd from $HOME.

\b NCSA \b Blue waters:
~~~~
module unload boost
module unload cmake
module load bwpy
export SOFTWARE_ROOT=${HOME}
export CPATH="${BWPY_DIR}/usr/include"
export LIBRARY_PATH="${BWPY_DIR}/lib64:${BWPY_DIR}/usr/lib64"
export LD_LIBRARY_PATH="${BWPY_DIR}/lib64:${BWPY_DIR}/usr/lib64"
~~~~

You must specify BOOST_ROOT manually on the cmake command line. You can select python2 or python3.
``cmake ~/hoomd -DPYTHON_EXECUTABLE=`which python3` -DBOOST_ROOT=${BWPY_DIR} -DCMAKE_INSTALL_PREFIX=$HOME/hoomd-install``

To run hoomd on blue waters, set PYTHONPATH=$PYTHONPATH:$HOME/hoomd-install/lib/hoomd/python-module, and run `ibrun <ibrun parameters> python3 script.py`.

\b OLCF \b Titan:
~~~~
module unload PrgEnv-pgi
module load PrgEnv-gnu
module load cmake/2.8.11.2
module load git
module load cudatoolkit
module load python/3.4.3
module load python_numpy/1.9.2
module load boost/1.60.0
# need gcc first on the search path
module unload gcc/4.9.0
module load gcc/4.9.0
export SOFTWARE_ROOT=${PROJWORK}/${your_project}/software/titan
~~~~

For more information, see: https://www.olcf.ornl.gov/support/system-user-guides/titan-user-guide/

\b OLCF \b Eos:

~~~~
module unload PrgEnv-intel
module load PrgEnv-gnu
module load cmake
module load git
module load python/3.4.3
module load python_numpy/1.9.2
module load boost/1.60.0
export SOFTWARE_ROOT=${PROJWORK}/${your_project}/software/eos
# need gcc first on the search path
module unload gcc/4.9.0
module load gcc/4.9.0

export CC="cc -dynamic"
export CXX="CC -dynamic"
~~~~

For more information, see: https://www.olcf.ornl.gov/support/system-user-guides/eos-user-guide/

\b XSEDE \b SDSC \b Comet:

~~~~
module purge
module load python
module unload intel
module load intel/2015.2.164
module load mvapich2_ib
module load gnutools
module load scipy
module load cmake
module load cuda/7.0
# module load boost/1.55.0

export CC=`which icc`
export CXX=`which icpc`
export SOFTWARE_ROOT=/oasis/projects/nsf/${your_project}/${USER}/software
~~~~

Comet's boost module exists and has boost::python, but it is non-functional. You need to build boost: see \ref sec_building_boost.

Note: The python module on comet provides both python2. You need to force hoomd to build
against python2: `cmake $HOME/devel/hoomd -DPYTHON_EXECUTABLE=`which python2`

Note: CUDA libraries are only available on GPU nodes on Comet. To run on the CPU-only nodes, you must build hoomd
with ENABLE_CUDA=off.

Note: Make sure to set CC and CXX. Without these, cmake will use /usr/bin/gcc and compilation will fail.

For more information, see: http://www.sdsc.edu/support/user_guides/comet.html

\b XSEDE \b SDSC \b Stampede:

~~~~~
module unload mvapich
module load intel/15.0.2
module load impi
module load cuda/7.0
module load cmake
module load git
module load python/2.7.9

export CC=`which icc`
export CXX=`which icpc`
export SOFTWARE_ROOT=${WORK}/software
~~~~~

Stampede's boost module does not include boost::python, you need to build boost: see \ref sec_building_boost.

Note: Stampede admins highly recommend building with the intel compiler and MPI libraries. They attribute random crashes
to the mvapich library and GNU compiler.

Note: CUDA libraries are only available on GPU nodes on Stampede. To run on the CPU-only nodes, you must build hoomd
with ENABLE_CUDA=off.

Note: Make sure to set CC and CXX. Without these, cmake will use /usr/bin/gcc and compilation will fail.

For more information, see: https://portal.tacc.utexas.edu/user-guides/stampede

\b STFC \b DiRAC \b Cambridge \b Darwin and \b Wilkes:

If you are running on Darwin and will not be using GPUs:
~~~~
. /etc/profile.d/modules.sh
module purge
module load default-impi
module load cmake
module load python/2.7.10
module load boost/1.60/gcc-5.2.0-python-2.7.10

export CC=`which gcc`
export CXX=`which c++`
export SOFTWARE_ROOT=/scratch/$USER/software
~~~~

To build, include the following additional `cmake` options.
~~~~
-DPYTHON_EXECUTABLE=`which python` \
-DBOOST_ROOT=$BOOST_HOME \
-DENABLE_MPI=ON \
-DBOOST_LIBRARYDIR=${BOOST_HOME}/lib \
-DBoost_REALPATH=ON \
-DMPIEXEC=`which mpirun`
~~~~

If you are running on Wilkes, you will need to include CUDA support:

~~~~
. /etc/profile.d/modules.sh
module purge
module load default-impi
module load cmake
module load gcc/4.9.2
module load python/2.7.5
module load boost/1.55/boost_1.55.0-gcc-python_2.7.5
module load cuda

export CC=`which gcc`
export CXX=`which c++`
export SOFTWARE_ROOT=/scratch/$USER/software
~~~~

To build, include the following additional `cmake` options.

~~~~
-DPYTHON_EXECUTABLE=`which python` \
-DHOOMD_PYTHON_LIBRARY=/usr/local/Cluster-Apps/python/2.7.5/lib64/libpython2.7.so \
-DBOOST_ROOT=$BOOST_HOME \
-DENABLE_MPI=ON \
-DBOOST_LIBRARYDIR=${BOOST_HOME}/lib \
-DBoost_REALPATH=ON \
-DMPIEXEC=`which mpirun`
~~~~

Note that the Darwin and Wilkes clusters have the same software environment
and shared filesystems, so you can build for Wilkes and use on Darwin.
However, as of March 2016, module incompatibilities necessitate older modules
and a quirk in the python installation requires explicitly setting the `libpython` location.

\subsection sec_building_boost Building boost on clusters

Not all clusters have a functioning boost::python library. On these systems, you will need to build your own boost
library. Download and unpack the latest version of the boost source code.

Then run the following in the shell. The variables are set for Comet, you will need to change the python version
and root directory to match your cluster.

~~~~
PREFIX="${SOFTWARE_ROOT}/boost"
PY_VER="2.7"
PYTHON="/opt/python/bin/python2.7"
PYTHON_ROOT="/opt/python"

./bootstrap.sh \
        --prefix="${PREFIX}" \
        --with-python="${PYTHON}" \
        --with-python-root="${PYTHON_ROOT} : ${PYTHON_ROOT}/include/python${PY_VER}m ${PYTHON_ROOT}/include/python${PY_VER}"

./b2 -q \
        --ignore-site-config \
        variant=release \
        architecture=x86 \
        debug-symbols=off \
        threading=multi \
        runtime-link=shared \
        link=shared \
        toolset=gcc \
        python="${PY_VER}" \
        --layout=system \
        -j20 \
        install
~~~~

Then set BOOST_ROOT=${SOFTWARE_ROOT}/boost before running cmake.

\subsection sec_building_workstation Installing prerequisites on a workstation

On your workstation, use your systems package manager to install all of the prerequisite libraries. Some linux
distributions separate `-dev` and normal packages, you need the development packages to build hoomd. Also, many linux
distributions ship both python2 and python3, but only build boost against python2. On such systems, you need to force
hoomd to build against python2. Check the hoomd-users mailing lists for posts by users who share their hoomd build
instructions on a variety of distributions.
<hr>
\section sec_build_linux_generic_compile Compile HOOMD-blue

Clone the git repository to get the source:
~~~~
$ git clone https://bitbucket.org/glotzer/hoomd-blue
~~~~
By default, the *maint* branch will be checked out. This branch includes all bug fixes since the last stable release.

Compile:
~~~~
$ cd hoomd-blue
$ mkdir build
$ cd build
$ cmake ../ -DCMAKE_INSTALL_PREFIX=${SOFTWARE_ROOT}/hoomd
$ make -j20
~~~~
Note: for development, you do not need to `make install`. `make` builds a functioning hoomd install:
launch `python-runner/hoomd`.

Run:
~~~~
$ make test
~~~~
to test your build

To install a stable version for general use, run `make install` and add ${SOFTWARE_ROOT}/hoomd/bin to your PATH.
<hr>

\section sec_build_linux_generic_mpi Compiling with MPI enabled

### System provided MPI

If your cluster administrator provides an installation of MPI, you need to figure out if is in your
`$PATH`. If the command
~~~~
$ which mpicc
/usr/bin/mpicc
~~~~
succeeds, you're all set. HOOMD-blue should detect your MPI compiler automatically.

If this is not the case, set the `MPI_HOME` environment variable to the location of the MPI installation.
~~~~
$ echo ${MPI_HOME}
/home/software/rhel5/openmpi-1.4.2/gcc
~~~~

### Build hoomd

Configure and build HOOMD-blue as normal (see \ref sec_build_linux_generic_compile). During the cmake step, MPI should
be detected and enabled. For cuda-aware MPI, additionally supply the **ENABLE_MPI_CUDA=ON** option to cmake.

<hr>
\section sec_build_options Build options
Here is a list of all the build options that can be changed by CMake. To changes these settings, cd to your <i>build</i>
directory and run
~~~~
$ ccmake .
~~~~
After changing an option, press *c* to configure then press *g* to generate. The makefile/IDE project is now updated with
the newly selected options. Alternately, you can set these parameters on the initial cmake invocation:
`cmake $HOME/devel/hoomd -DENABLE_CUDA=off`

Options that specify library versions only take effect on a clean invocation of cmake. To set these options, first
remove `CMakeCache.txt` and then run cmake and specify these options on the command line.
- **PYTHON_EXECUTABLE** - Specify python to build against. Example: /usr/bin/python2
- **BOOST_ROOT** - Specify root directory to search for boost. Example: /sw/rhel7/boost-1.60.0

Other option changes take effect at any time. These can be set from within `ccmake` or on the command line.
- **BUILD_TESTING** - Enables the compilation of unit tests
- **CMAKE_BUILD_TYPE** - sets the build type (case sensitive)
    - **Debug** - Compiles debug information into the library and executables.
       Enables asserts to check for programming mistakes. HOOMD-blue will run
       \e very slow if compiled in Debug mode, but problems are easier to
       identify.
    - **RelWithDebInfo** - Compiles with optimizations and debug symbols. Useful for profiling benchmarks.
    - **Release** - All compiler optimizations are enabled and asserts are removed.
       Recommended for production builds: required for any benchmarking.
- **ENABLE_CUDA** - Enable compiling of the GPU accelerated computations using CUDA. Defaults *on* if the CUDA toolkit
    is found. Defaults *off* if the CUDA toolkit is not found.
- **ENABLE_DOXYGEN** - enables the generation of user and developer documentation (Defaults *off*)
- **SINGLE_PRECISION** - Controls precision
    - When set to \b ON, all calculations are performed in single precision.
    - When set to \b OFF, all calculations are performed in double precision.
- **ENABLE_MPI** - Enable multi-processor/GPU simulations using MPI
    - When set to \b ON (default if any MPI library is found automatically by CMake), multi-GPU simulations are supported
    - When set to \b OFF, HOOMD always runs in single-GPU mode
- **ENABLE_MPI_CUDA** - Enable CUDA-aware MPI library support
    - Requires a MPI library with CUDA support to be installed
    - When set to \b ON (default if a CUDA-aware MPI library is detected), HOOMD-blue will make use of  the capability of the MPI library to accelerate CUDA-buffer transfers
    - When set to \b OFF, standard MPI calls will be used
    - *Warning:* Manually setting this feature to ON when the MPI library does not support CUDA may
      result in a crash of HOOMD-blue

These options control CUDA compilation:
- **CUDA_ARCH_LIST** - A semicolon separated list of GPU architecture to compile in.    Portions of HOOMD are optimized for specific
                   hardware architectures, but those optimizations are only activated when they are compiled in.
                   By default, all known architectures supported by the installed CUDA toolkit are activated in the list.
                   There is no disadvantage to doing so, except perhaps a slightly larger executable size and compile times.
                   The CUDA programming guide contains list of which GPUs are which compute version in Appendix A.
                   Note: nvcc does not treat sm_21 differently from sm_20. 21
                   should not be added to CUDA_ARCH_LIST.
- **NVCC_FLAGS** - Allows additional flags to be passed to nvcc.

\section sec_build_plugin Building a plugin for HOOMD-blue

There are several methods that can be used to build code that interfaces with hoomd.

### Method 1: Write a full-fledged plugin in python only

Some plugins can be implemented fully in python, providing high-level code for configuring or running simulations.

In order to use such a plugin, one must first:
-# Compile hoomd normally
-# `make install` hoomd to a desired location
-# Add `hoomd_install_location/bin` to your `PATH` as usual

Create a directory to contain the python module for the plugin:
~~~~
cd hoomd_install_location/lib/hoomd/python-module/hoomd_plugins
mkdir plugin_name
cd plugin_name
touch __init__.py
~~~~

You should develop your plugin in a directory outside hoomd_install_location
and using a revision control software. You would not want to loose the code you've written when hoomd is uninstalled!
In this case, you can just copy the module to the hoomd-plugins directory to install it.
~~~~
cp -R plugin_name hoomd_install_location/lib/hoomd/python-module/hoomd_plugins
~~~~

Once the plugin is written and installed, it can be used in a hoomd script like so:
~~~~
from hoomd_script import *
from hoomd_plugins import plugin_name

init.whatever(...)
plugin_name.whatever(...)
~~~~

### Method 2: Write a full-fledged plugin with C++ code included

For high performance, execution on the GPU, or other reasons, part of a plugin can be written in C++. To write a plugin
that incorporates such code, `make install` hoomd as normal. Then copy the
directory `hoomd_install_location/share/hoomd/plugin_template_cpp` to a new working space and modify it to implement your
plugin. See the README file in that directory for full documentation. Examples of new pair and bond potentials
are available in `hoomd_install_location/share/hoomd/plugin_template_evaluators_ext`

*/
